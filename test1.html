<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>AR Testing — Composite Recording (Camera + AR)</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- A-Frame + extras + MindAR -->
    <script src="https://aframe.io/releases/1.5.0/aframe.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/donmccurdy/aframe-extras@v7.0.0/dist/aframe-extras.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mind-ar@1.2.5/dist/mindar-image-aframe.prod.js"></script>

    <style>
      html, body { margin:0; padding:0; height:100%; overflow:hidden; font-family:sans-serif; }
      #start-overlay{
        position:fixed; inset:0; background:#000; color:#fff; z-index:999;
        display:flex; flex-direction:column; justify-content:center; align-items:center;
      }
      #start-button{
        padding:14px 28px; font-size:16px; background:#00cec9; color:#fff;
        border:0; border-radius:6px; cursor:pointer; margin-top:12px;
      }
      #rec-controls{
        position:fixed; bottom:12px; left:12px; z-index:9999;
        display:flex; gap:8px; align-items:center;
        background:rgba(0,0,0,.55); color:#fff; padding:8px 10px; border-radius:10px;
      }
      #rec-toggle{
        padding:10px 14px; border:0; border-radius:6px; background:#e63946; color:#fff; cursor:pointer;
      }
      #dl{
        background:#0a7; color:#fff; padding:8px 12px; border-radius:6px; text-decoration:none; display:none;
      }
      #mic-label{ background:#000a; color:#fff; padding:6px 8px; border-radius:6px; }
    </style>
  </head>

  <body>
    <!-- Tap-to-start overlay (unlocks media on iOS) -->
    <div id="start-overlay">
      <h1>Tap to Start AR Experience</h1>
      <button id="start-button">Start</button>
    </div>

    <!-- Composite-record controls (camera + AR overlays) -->
    <div id="rec-controls">
      <button id="rec-toggle">● Record Camera + AR</button>
      <label id="mic-label"><input id="include-mic" type="checkbox"> Include mic</label>
      <a id="dl" download>Download</a>
    </div>

    <!-- Hidden mixer canvas (we record this) -->
    <canvas id="mix-canvas" style="display:none;"></canvas>

    <!-- A-Frame Scene -->
    <a-scene
      mindar-image="imageTargetSrc: ./targets.mind; autoStart: true; uiScanning: true; uiLoading: true; filterMinCF:0.05; filterBeta:40; warmupTolerance:3; missTolerance:3"
      embedded
      color-space="sRGB"
      renderer="colorManagement: true, physicallyCorrectLights"
      vr-mode-ui="enabled: false"
      device-orientation-permission-ui="enabled: true"
      id="ar-scene"
    >
      <a-assets>
        <!-- Overlay video (NOT the camera) -->
        <video
          id="ad-video"
          src="./video.mp4"
          preload="auto"
          crossorigin="anonymous"
          playsinline
          webkit-playsinline
          controls
        ></video>
      </a-assets>

      <a-camera position="0 0 0" look-controls="enabled: false"></a-camera>

      <a-entity mindar-image-target="targetIndex: 0" id="video-target">
        <a-video
          id="video-plane"
          src="#ad-video"
          width="1.3"
          height="1.7"
          position="0 0 0"
          rotation="0 0 0"
        ></a-video>
      </a-entity>
    </a-scene>

    <script>
      const startButton = document.getElementById("start-button");
      const overlay = document.getElementById("start-overlay");
      const sceneEl = document.getElementById("ar-scene");
      const video = document.getElementById("ad-video");
      const target = document.getElementById("video-target");

      const recBtn  = document.getElementById('rec-toggle');
      const dl      = document.getElementById('dl');
      const micBox  = document.getElementById('include-mic');
      const mixCanvas = document.getElementById('mix-canvas');
      let mixCtx, rafId;

      let audioUnlocked = false;
      let cameraVideoEl = null;   // MindAR's hidden <video> with srcObject MediaStream
      let mediaRecorder, chunks = [];
      let audioCtx, dest, micStream = null;
      let recording = false;

      // *** Fix: prevent A-Frame canvas from intercepting the Start tap ***
      sceneEl.style.pointerEvents = "none";

      // Unlock media on user gesture; keep overlay video paused until target is found
      startButton.addEventListener("click", async () => {
        try { await video.play(); } catch(e) {}
        video.pause();
        audioUnlocked = true;
        overlay.style.display = "none";
        // re-enable scene interactions after user gesture
        sceneEl.style.pointerEvents = "auto";
      });

      // Play/pause overlay asset based on marker visibility
      target.addEventListener("targetFound", () => { if (audioUnlocked) video.play(); });
      target.addEventListener("targetLost",  () => { video.pause(); });

      // MindAR ready -> find the camera <video> MindAR created
      sceneEl.addEventListener("arReady", () => {
        const vids = Array.from(document.querySelectorAll('video'));
        cameraVideoEl = vids.find(v => v !== video && v.srcObject instanceof MediaStream);
        if (!cameraVideoEl) {
          console.warn("MindAR camera <video> not found yet; recording will work once it's present.");
        } else {
          const st = cameraVideoEl.srcObject.getVideoTracks()[0]?.getSettings?.() || {};
          console.log("Camera track:", st.width, "x", st.height, st.frameRate || "fps?");
        }
      });

      function pickMime() {
        const prefs = [
          'video/mp4;codecs=avc1.42E01E,mp4a.40.2',
          'video/webm;codecs=vp9,opus',
          'video/webm;codecs=vp8,opus',
          'video/webm'
        ];
        return prefs.find(t => window.MediaRecorder && MediaRecorder.isTypeSupported(t)) || '';
      }

      function sizeMixCanvas() {
        const camW = cameraVideoEl?.videoWidth || 0;
        const camH = cameraVideoEl?.videoHeight || 0;
        const sceneCanvas = sceneEl.canvas;
        const w = camW || sceneCanvas?.width || 1280;
        const h = camH || sceneCanvas?.height || 720;
        mixCanvas.width = w;
        mixCanvas.height = h;
        if (!mixCtx) mixCtx = mixCanvas.getContext('2d', { alpha: true });
      }

      function drawComposite() {
        const sceneCanvas = sceneEl.canvas;
        if (!cameraVideoEl || !sceneCanvas) { rafId = requestAnimationFrame(drawComposite); return; }

        try {
          // Draw camera full frame
          mixCtx.drawImage(cameraVideoEl, 0, 0, mixCanvas.width, mixCanvas.height);
          // Draw AR overlays (WebGL canvas) on top
          mixCtx.drawImage(sceneCanvas, 0, 0, mixCanvas.width, mixCanvas.height);
        } catch (e) {
          console.warn('Composite draw failed (CORS/taint?):', e);
        }
        rafId = requestAnimationFrame(drawComposite);
      }

      async function startCompositeRecording() {
        if (!cameraVideoEl || !(cameraVideoEl.srcObject instanceof MediaStream) || !sceneEl.canvas) {
          alert('Camera/scene not ready yet — try again in a second.');
          return;
        }

        // Size mixer to camera (preferred) or scene
        sizeMixCanvas();

        // Start composite draw loop
        cancelAnimationFrame(rafId);
        drawComposite();

        // Capture composite video from the mixer canvas
        const videoTrack = mixCanvas.captureStream(30).getVideoTracks()[0];

        // Build audio mix (overlay video audio + optional mic)
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        dest     = audioCtx.createMediaStreamDestination();

        // Route overlay <video> audio to recorder (and to device speakers, optional)
        const vSrc = audioCtx.createMediaElementSource(video);
        vSrc.connect(dest);
        vSrc.connect(audioCtx.destination);

        if (micBox.checked) {
          try {
            micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
            const mSrc = audioCtx.createMediaStreamSource(micStream);
            mSrc.connect(dest);
          } catch (e) {
            console.warn('Mic denied/failed:', e);
          }
        }

        // Combine composite video + mixed audio
        const outStream = new MediaStream([videoTrack, ...dest.stream.getAudioTracks()]);
        const mime = pickMime();

        mediaRecorder = new MediaRecorder(outStream, {
          mimeType: mime,
          videoBitsPerSecond: 8_000_000
        });
        chunks = [];

        mediaRecorder.ondataavailable = e => { if (e.data && e.data.size) chunks.push(e.data); };
        mediaRecorder.onstop = () => {
          const blob = new Blob(chunks, { type: mediaRecorder.mimeType || 'video/webm' });
          const url = URL.createObjectURL(blob);
          dl.href = url;
          dl.download = (mediaRecorder.mimeType || '').includes('mp4') ? 'ar-composite.mp4' : 'ar-composite.webm';
          dl.style.display = 'inline-block';

          // Cleanup
          cancelAnimationFrame(rafId);
          if (micStream) { micStream.getTracks().forEach(t => t.stop()); micStream = null; }
          try { audioCtx.close(); } catch(_) {}
          recording = false;
          recBtn.textContent = '● Record Camera + AR';
        };

        // iOS: resume AudioContext on user gesture if needed
        if (audioCtx.state === 'suspended') await audioCtx.resume();

        mediaRecorder.start();
        recording = true;
        dl.style.display = 'none';
        recBtn.textContent = '⏹ Stop';
      }

      function stopCompositeRecording() {
        if (mediaRecorder && recording) mediaRecorder.stop();
      }

      recBtn.addEventListener('click', () => {
        if (!recording) startCompositeRecording(); else stopCompositeRecording();
      });
    </script>

    <!-- === Smoothing wrapper (adds a smoothed follower for the video plane) === -->
    <script>
      (function () {
        const THREE = window.THREE;
        const sceneEl = document.getElementById('ar-scene');
        const targetEl = document.getElementById('video-target');
        const planeEl  = document.getElementById('video-plane');

        let anchor3D, scene3D, smoothGroup;
        let smoothPos, smoothQuat, smoothScale;
        let targetPos, targetQuat, targetScale;
        let initialized = false;
        let raf;

        function setup() {
          anchor3D = targetEl.object3D;
          scene3D  = sceneEl.object3D;

          // Create a smoothing group at scene root and reparent the video-plane under it
          smoothGroup = new THREE.Group();
          smoothGroup.visible = false; // becomes visible on first targetFound
          scene3D.add(smoothGroup);
          // Move the A-Frame entity's object3D under the smoothing group
          smoothGroup.add(planeEl.object3D);

          // Prepare reusable vectors/quaternions
          smoothPos  = new THREE.Vector3();
          smoothQuat = new THREE.Quaternion();
          smoothScale= new THREE.Vector3(1,1,1);
          targetPos  = new THREE.Vector3();
          targetQuat = new THREE.Quaternion();
          targetScale= new THREE.Vector3(1,1,1);

          initialized = true;
          loop();
        }

        function loop() {
          const alpha = 0.15; // increase to 0.20 if still jittery
          if (initialized) {
            // get anchor world transform
            anchor3D.updateWorldMatrix(true, false);
            const m = anchor3D.matrixWorld;
            m.decompose(targetPos, targetQuat, targetScale);

            if (!smoothGroup.visible) {
              // first-frame snap when coming from lost->found
              smoothPos.copy(targetPos);
              smoothQuat.copy(targetQuat);
              smoothScale.copy(targetScale);
            } else {
              // smooth follow (position, rotation, scale)
              smoothPos.lerp(targetPos, alpha);
              THREE.Quaternion.slerp(smoothQuat, targetQuat, smoothQuat, alpha);
              smoothScale.lerp(targetScale, alpha);
            }

            smoothGroup.position.copy(smoothPos);
            smoothGroup.quaternion.copy(smoothQuat);
            smoothGroup.scale.copy(smoothScale);
          }
          raf = requestAnimationFrame(loop);
        }

        // Wait for renderer to start so object3D graph exists
        sceneEl.addEventListener('renderstart', setup, { once: true });

        // Sync visibility with tracking state
        targetEl.addEventListener('targetFound', () => {
          if (smoothGroup) smoothGroup.visible = true;
        });
        targetEl.addEventListener('targetLost', () => {
          if (smoothGroup) smoothGroup.visible = false;
        });
      })();
    </script>
  </body>
</html>
